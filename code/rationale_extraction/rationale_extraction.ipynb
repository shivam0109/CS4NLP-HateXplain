{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0169911e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from email import header\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from hugchat import hugchat\n",
    "import pickle \n",
    "from hugchat.login import Login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db5fb49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/shivamadarsh/miniconda3/lib/python3.9/site-packages/hugchat/usercookies/shivam.adarsh@uzh.ch.json'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# login\n",
    "email = \"shivam.adarsh@uzh.ch\"\n",
    "passwd = \"\" # Your huggingface password here \n",
    "sign = Login(email, passwd)\n",
    "# load cookies from usercookies/<email>.json\n",
    "cookies = sign.loadCookies() # This will detect if the JSON file exists, return cookies if it does and raise an Exception if it's not.\n",
    "#cookies = sign.login()\n",
    "sign.saveCookies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94e90878",
   "metadata": {},
   "outputs": [],
   "source": [
    "## INPUT FILES \n",
    "train_path = '../../data/load_and_preprocess/df_train_processed.csv'\n",
    "val_path = '../../data/load_and_preprocess/df_val_processed.csv'\n",
    "test_path = '../../data/load_and_preprocess/df_test_processed.csv'\n",
    "\n",
    "## OUTPUT FILES \n",
    "train_rationale_path = '../data/rationale_extraction/df_train_with_rationales.csv'\n",
    "val_rationale_path = '../data/rationale_extraction/df_val_with_rationales.csv'\n",
    "test_rationale_path = '../data/rationale_extraction/df_test_with_rationales.csv'\n",
    "texts_not_processed_path = '../data/texts_not_processed.pkl'\n",
    "\n",
    "# Prompt \n",
    "prompt_start = \"\"\"\n",
    "Your task is to classify a sentence into hate speech, offensive speech or normal speech, and explain your answer. In addition, you would need to provide the communities being targeted and the keywords for your answer present in the sentence.\n",
    "\n",
    "Produce the result in four parts - your first part should give the classification. \n",
    "The second part should give the community being targeted from the following list: ['African', 'Islam', 'Jewish', 'Homosexual', 'Women', 'Refugee', 'Arab', 'Caucasian', 'Asian', 'Hispanic', 'Other community']. You can select multiple communities if it's relevant. \n",
    "The third part should give an explanation of your answer. \n",
    "The fourth part should extract the key words only from the sentence due to which the sentence might be offensive or hate-speech. Don't include words not present in the sentence.\n",
    "\n",
    "Here is an example: \n",
    "Input Sentence: Muslims should be banned from praying on the streets \n",
    "\n",
    "Output Start : \n",
    "Classification: Offensive\n",
    "Community targeted: Islam \n",
    "Explanation: Using the word \"ban\" is authoritarian, and hurts the sentiments of people following that religion. The phrasing lacks justification or evidence for why this particular group should face restrictions and creates tension between majority cultural values and religious freedoms enjoyed by all citizens. Therefore, this statement reflects a narrow perspective about diversity and social cohesion within our societies.\n",
    "Keywords: banned, prayers\n",
    "Output End\n",
    "\n",
    "Now, try to produce the output in a similar format for the following sentence: \n",
    "Sentence: \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2a9ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtractRationales():\n",
    "    def __init__(self):\n",
    "        self.df_train = pd.read_csv(train_path)\n",
    "        self.df_val = pd.read_csv(val_path)\n",
    "        self.df_test = pd.read_csv(test_path)\n",
    "        self.prompt_start = prompt_start\n",
    "        self.chatbot = hugchat.ChatBot(cookies=cookies.get_dict())\n",
    "        self.df_train_out = None\n",
    "        try:\n",
    "            self.df_train_out = pd.read_csv(train_rationale_path)\n",
    "            print(\"Some train output already exists\")\n",
    "        except:\n",
    "            print(\"No train output exists\")\n",
    "        if self.df_train_out is not None:\n",
    "            left_ids = list(set(self.df_train['id']).difference(set(self.df_train_out['id'])))\n",
    "            self.df_train = self.df_train[self.df_train['id'].isin(left_ids)]\n",
    "            print(\"Remaning data: \", len(left_ids))\n",
    "        \n",
    "\n",
    "    ## Helper function to get Hugchat Responses\n",
    "    def get_hugchat_response_helper(self, text):\n",
    "        self.not_processed = [] \n",
    "        prompt = self.prompt_start + text \n",
    "        try:\n",
    "            response = self.chatbot.chat(prompt)\n",
    "        except:\n",
    "            response = \"\" \n",
    "            self.not_processed.append(text)\n",
    "        return response \n",
    "\n",
    "    # Function to split df into 10 chunks - easier to extract and write on disk. \n",
    "    def chunk_data(self, df):\n",
    "        if df.shape[0]>=10000:\n",
    "            return np.array_split(df, 1000)\n",
    "        else:\n",
    "            return np.array_split(df, 100)\n",
    "\n",
    "    # Function to save generated rationales for each df chunk. \n",
    "    def save_rationales(self, df, path, mode='a'):\n",
    "        df['hugchat_response'] = df['unmasked_sentence'].apply(lambda x: self.get_hugchat_response_helper(x))\n",
    "        if mode=='a':\n",
    "            df.to_csv(path, index=False, mode=mode, header=False)\n",
    "        else:\n",
    "            df.to_csv(path, index=False, mode=mode)\n",
    "\n",
    "    # Function to loop over df_train_list or df_val_list or df_test_list\n",
    "    def loop_over_chunk(self, df_list, path):\n",
    "        for i in range(len(df_list)):\n",
    "            print(\"Chunk: \", i)\n",
    "            if self.df_train_out is not None:\n",
    "                self.save_rationales(df_list[i], path, 'a')\n",
    "            elif i==0:\n",
    "                self.save_rationales(df_list[i], path, 'w')\n",
    "            else:\n",
    "                self.save_rationales(df_list[i], path, 'a')\n",
    "\n",
    "    ## Function to get HugChat Responses for train, val and test datasets. \n",
    "    def get_hugchat_response(self):\n",
    "        # Chunk data and create a list of dfs \n",
    "        df_train_list = self.chunk_data(self.df_train)\n",
    "        df_val_list = self.chunk_data(self.df_val)\n",
    "        df_test_list = self.chunk_data(self.df_test)\n",
    "        \n",
    "        # Get Rationales for all dataframes \n",
    "        print(\"Training Data: \\n\")\n",
    "        self.loop_over_chunk(df_train_list, train_rationale_path)\n",
    "        print(\"Validation Data: \\n\")\n",
    "        self.loop_over_chunk(df_val_list, val_rationale_path)\n",
    "        print(\"Test Data: \\n\")\n",
    "        self.loop_over_chunk(df_test_list, test_rationale_path)\n",
    "        \n",
    "        # Save texts that were not processed \n",
    "        with open(texts_not_processed_path, 'wb') as f:\n",
    "            pickle.dump(self.not_processed, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf1687e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some train output already exists\n",
      "Remaning data:  1322\n",
      "Training Data: \n",
      "\n",
      "Chunk:  0\n",
      "Chunk:  1\n",
      "Chunk:  2\n",
      "Chunk:  3\n",
      "Chunk:  4\n",
      "Chunk:  5\n",
      "Chunk:  6\n",
      "Chunk:  7\n",
      "Chunk:  8\n",
      "Chunk:  9\n",
      "Chunk:  10\n",
      "Chunk:  11\n",
      "Chunk:  12\n",
      "Chunk:  13\n",
      "Chunk:  14\n",
      "Chunk:  15\n",
      "Chunk:  16\n",
      "Chunk:  17\n",
      "Chunk:  18\n",
      "Chunk:  19\n",
      "Chunk:  20\n",
      "Chunk:  21\n",
      "Chunk:  22\n",
      "Chunk:  23\n",
      "Chunk:  24\n",
      "Chunk:  25\n",
      "Chunk:  26\n",
      "Chunk:  27\n",
      "Chunk:  28\n",
      "Chunk:  29\n",
      "Chunk:  30\n",
      "Chunk:  31\n",
      "Chunk:  32\n",
      "Chunk:  33\n",
      "Chunk:  34\n",
      "Chunk:  35\n",
      "Chunk:  36\n",
      "Chunk:  37\n",
      "Chunk:  38\n",
      "Chunk:  39\n",
      "Chunk:  40\n",
      "Chunk:  41\n",
      "Chunk:  42\n",
      "Chunk:  43\n",
      "Chunk:  44\n",
      "Chunk:  45\n",
      "Chunk:  46\n",
      "Chunk:  47\n",
      "Chunk:  48\n",
      "Chunk:  49\n",
      "Chunk:  50\n",
      "Chunk:  51\n",
      "Chunk:  52\n",
      "Chunk:  53\n",
      "Chunk:  54\n",
      "Chunk:  55\n",
      "Chunk:  56\n",
      "Chunk:  57\n",
      "Chunk:  58\n",
      "Chunk:  59\n",
      "Chunk:  60\n",
      "Chunk:  61\n",
      "Chunk:  62\n",
      "Chunk:  63\n",
      "Chunk:  64\n",
      "Chunk:  65\n",
      "Chunk:  66\n",
      "Chunk:  67\n",
      "Chunk:  68\n",
      "Chunk:  69\n",
      "Chunk:  70\n",
      "Chunk:  71\n",
      "Chunk:  72\n",
      "Chunk:  73\n",
      "Chunk:  74\n",
      "Chunk:  75\n",
      "Chunk:  76\n",
      "Chunk:  77\n",
      "Chunk:  78\n",
      "Chunk:  79\n",
      "Chunk:  80\n",
      "Chunk:  81\n",
      "Chunk:  82\n",
      "Chunk:  83\n",
      "Chunk:  84\n",
      "Chunk:  85\n",
      "Chunk:  86\n",
      "Chunk:  87\n",
      "Chunk:  88\n",
      "Chunk:  89\n",
      "Chunk:  90\n",
      "Chunk:  91\n",
      "Chunk:  92\n",
      "Chunk:  93\n",
      "Chunk:  94\n",
      "Chunk:  95\n",
      "Chunk:  96\n",
      "Chunk:  97\n",
      "Chunk:  98\n",
      "Chunk:  99\n",
      "Validation Data: \n",
      "\n",
      "Chunk:  0\n",
      "Chunk:  1\n",
      "Chunk:  2\n",
      "Chunk:  3\n",
      "Chunk:  4\n",
      "Chunk:  5\n",
      "Chunk:  6\n",
      "Chunk:  7\n",
      "Chunk:  8\n",
      "Chunk:  9\n",
      "Chunk:  10\n",
      "Chunk:  11\n",
      "Chunk:  12\n",
      "Chunk:  13\n",
      "Chunk:  14\n",
      "Chunk:  15\n",
      "Chunk:  16\n",
      "Chunk:  17\n",
      "Chunk:  18\n",
      "Chunk:  19\n",
      "Chunk:  20\n",
      "Chunk:  21\n",
      "Chunk:  22\n",
      "Chunk:  23\n",
      "Chunk:  24\n",
      "Chunk:  25\n",
      "Chunk:  26\n",
      "Chunk:  27\n",
      "Chunk:  28\n",
      "Chunk:  29\n",
      "Chunk:  30\n",
      "Chunk:  31\n",
      "Chunk:  32\n",
      "Chunk:  33\n",
      "Chunk:  34\n",
      "Chunk:  35\n",
      "Chunk:  36\n",
      "Chunk:  37\n",
      "Chunk:  38\n",
      "Chunk:  39\n",
      "Chunk:  40\n",
      "Chunk:  41\n",
      "Chunk:  42\n",
      "Chunk:  43\n",
      "Chunk:  44\n",
      "Chunk:  45\n",
      "Chunk:  46\n",
      "Chunk:  47\n",
      "Chunk:  48\n",
      "Chunk:  49\n",
      "Chunk:  50\n",
      "Chunk:  51\n",
      "Chunk:  52\n",
      "Chunk:  53\n",
      "Chunk:  54\n",
      "Chunk:  55\n",
      "Chunk:  56\n",
      "Chunk:  57\n",
      "Chunk:  58\n",
      "Chunk:  59\n",
      "Chunk:  60\n",
      "Chunk:  61\n",
      "Chunk:  62\n",
      "Chunk:  63\n",
      "Chunk:  64\n",
      "Chunk:  65\n",
      "Chunk:  66\n",
      "Chunk:  67\n",
      "Chunk:  68\n",
      "Chunk:  69\n",
      "Chunk:  70\n",
      "Chunk:  71\n",
      "Chunk:  72\n",
      "Chunk:  73\n",
      "Chunk:  74\n",
      "Chunk:  75\n",
      "Chunk:  76\n",
      "Chunk:  77\n",
      "Chunk:  78\n",
      "Chunk:  79\n",
      "Chunk:  80\n",
      "Chunk:  81\n",
      "Chunk:  82\n",
      "Chunk:  83\n",
      "Chunk:  84\n",
      "Chunk:  85\n",
      "Chunk:  86\n",
      "Chunk:  87\n",
      "Chunk:  88\n",
      "Chunk:  89\n",
      "Chunk:  90\n",
      "Chunk:  91\n",
      "Chunk:  92\n",
      "Chunk:  93\n",
      "Chunk:  94\n",
      "Chunk:  95\n",
      "Chunk:  96\n",
      "Chunk:  97\n",
      "Chunk:  98\n",
      "Chunk:  99\n",
      "Test Data: \n",
      "\n",
      "Chunk:  0\n",
      "Chunk:  1\n",
      "Chunk:  2\n",
      "Chunk:  3\n",
      "Chunk:  4\n",
      "Chunk:  5\n",
      "Chunk:  6\n",
      "Chunk:  7\n",
      "Chunk:  8\n",
      "Chunk:  9\n",
      "Chunk:  10\n",
      "Chunk:  11\n",
      "Chunk:  12\n",
      "Chunk:  13\n",
      "Chunk:  14\n",
      "Chunk:  15\n",
      "Chunk:  16\n",
      "Chunk:  17\n",
      "Chunk:  18\n",
      "Chunk:  19\n",
      "Chunk:  20\n",
      "Chunk:  21\n",
      "Chunk:  22\n",
      "Chunk:  23\n",
      "Chunk:  24\n",
      "Chunk:  25\n",
      "Chunk:  26\n",
      "Chunk:  27\n",
      "Chunk:  28\n",
      "Chunk:  29\n",
      "Chunk:  30\n",
      "Chunk:  31\n",
      "Chunk:  32\n",
      "Chunk:  33\n",
      "Chunk:  34\n",
      "Chunk:  35\n",
      "Chunk:  36\n",
      "Chunk:  37\n",
      "Chunk:  38\n",
      "Chunk:  39\n",
      "Chunk:  40\n",
      "Chunk:  41\n",
      "Chunk:  42\n",
      "Chunk:  43\n",
      "Chunk:  44\n",
      "Chunk:  45\n",
      "Chunk:  46\n",
      "Chunk:  47\n",
      "Chunk:  48\n",
      "Chunk:  49\n",
      "Chunk:  50\n",
      "Chunk:  51\n",
      "Chunk:  52\n",
      "Chunk:  53\n",
      "Chunk:  54\n",
      "Chunk:  55\n",
      "Chunk:  56\n",
      "Chunk:  57\n",
      "Chunk:  58\n",
      "Chunk:  59\n",
      "Chunk:  60\n",
      "Chunk:  61\n",
      "Chunk:  62\n",
      "Chunk:  63\n",
      "Chunk:  64\n",
      "Chunk:  65\n",
      "Chunk:  66\n",
      "Chunk:  67\n",
      "Chunk:  68\n",
      "Chunk:  69\n",
      "Chunk:  70\n",
      "Chunk:  71\n",
      "Chunk:  72\n",
      "Chunk:  73\n",
      "Chunk:  74\n",
      "Chunk:  75\n",
      "Chunk:  76\n",
      "Chunk:  77\n",
      "Chunk:  78\n",
      "Chunk:  79\n",
      "Chunk:  80\n",
      "Chunk:  81\n",
      "Chunk:  82\n",
      "Chunk:  83\n",
      "Chunk:  84\n",
      "Chunk:  85\n",
      "Chunk:  86\n",
      "Chunk:  87\n",
      "Chunk:  88\n",
      "Chunk:  89\n",
      "Chunk:  90\n",
      "Chunk:  91\n",
      "Chunk:  92\n",
      "Chunk:  93\n",
      "Chunk:  94\n",
      "Chunk:  95\n",
      "Chunk:  96\n",
      "Chunk:  97\n",
      "Chunk:  98\n",
      "Chunk:  99\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    er = ExtractRationales()\n",
    "    er.get_hugchat_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a299ec67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
