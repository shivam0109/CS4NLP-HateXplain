{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3faeb41982864a388903f016144bd411": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0001e9c191174cbf95b54e9c270d589c",
              "IPY_MODEL_1e19468379f346b6aae2b3b858f865c3",
              "IPY_MODEL_e19201fa135b4dc68bb28739b2e426ef"
            ],
            "layout": "IPY_MODEL_98f6d3b49af0461496c0cc96c0041ddf"
          }
        },
        "0001e9c191174cbf95b54e9c270d589c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4277637e1ec54cc2834a9d1f207038e0",
            "placeholder": "​",
            "style": "IPY_MODEL_4c5c0bb9657a4b79b6c35f140ee9b178",
            "value": "Downloading (…)ve/main/spiece.model: 100%"
          }
        },
        "1e19468379f346b6aae2b3b858f865c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ba67575acff465f8ecdde91db9b3176",
            "max": 791656,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a1b365a5f39344638a904f570cd79945",
            "value": 791656
          }
        },
        "e19201fa135b4dc68bb28739b2e426ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f239b8f041af482d9ea959e332023113",
            "placeholder": "​",
            "style": "IPY_MODEL_5b0998509f0d4d16943b733d585306fa",
            "value": " 792k/792k [00:00&lt;00:00, 11.7MB/s]"
          }
        },
        "98f6d3b49af0461496c0cc96c0041ddf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4277637e1ec54cc2834a9d1f207038e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c5c0bb9657a4b79b6c35f140ee9b178": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba67575acff465f8ecdde91db9b3176": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a1b365a5f39344638a904f570cd79945": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f239b8f041af482d9ea959e332023113": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0998509f0d4d16943b733d585306fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "516a88f9de4848b5b1014ee1cfd9ed2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47abc5409d654a3eba35a6b14a1fcaf2",
              "IPY_MODEL_0277b84b6b1f41bf90e02c26aaae32ff",
              "IPY_MODEL_1061b007b7a94092af14d52ac14de938"
            ],
            "layout": "IPY_MODEL_0903df6829d04d2b8e900d3ab6a2fcbb"
          }
        },
        "47abc5409d654a3eba35a6b14a1fcaf2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efe0671c5848433fb0bddc9d35fea4dc",
            "placeholder": "​",
            "style": "IPY_MODEL_831a56ac485740d693453093319821b8",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "0277b84b6b1f41bf90e02c26aaae32ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fc369fd0a104b9d9f7823e533ffadab",
            "max": 1208,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27c1b42f280540a6adf055f5ae0f6176",
            "value": 1208
          }
        },
        "1061b007b7a94092af14d52ac14de938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0495b72c3ccc44238e3d6c86999eb07a",
            "placeholder": "​",
            "style": "IPY_MODEL_c5b847343fda40da8e2ed4150998318d",
            "value": " 1.21k/1.21k [00:00&lt;00:00, 79.7kB/s]"
          }
        },
        "0903df6829d04d2b8e900d3ab6a2fcbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efe0671c5848433fb0bddc9d35fea4dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "831a56ac485740d693453093319821b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8fc369fd0a104b9d9f7823e533ffadab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27c1b42f280540a6adf055f5ae0f6176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0495b72c3ccc44238e3d6c86999eb07a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c5b847343fda40da8e2ed4150998318d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] optuna sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpinTH6_OZB4",
        "outputId": "ae78e608-1e21-471b-c41d-e75bc426b6d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting optuna\n",
            "  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers[torch])\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers[torch])\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m78.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers[torch])\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.65.0)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.9 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.0.1+cu118)\n",
            "Collecting accelerate>=0.20.2 (from transformers[torch])\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n",
            "  Downloading cmaes-0.9.1-py3-none-any.whl (21 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.16)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.2->transformers[torch]) (5.9.5)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna)\n",
            "  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.6.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.9->transformers[torch]) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch!=1.12.0,>=1.9->transformers[torch]) (16.0.6)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.9->transformers[torch]) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.9->transformers[torch]) (1.3.0)\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, Mako, colorlog, cmaes, huggingface-hub, alembic, transformers, optuna, accelerate\n",
            "Successfully installed Mako-1.2.4 accelerate-0.20.3 alembic-1.11.1 cmaes-0.9.1 colorlog-6.7.0 huggingface-hub-0.16.4 optuna-3.2.0 safetensors-0.3.1 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2A22CgUOFtE",
        "outputId": "3a7a1dce-7b5e-4adc-c84b-52724161ee08"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.cuda.amp.autocast_mode.autocast at 0x7f0992245810>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# load packages\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import optuna\n",
        "from optuna.pruners import SuccessiveHalvingPruner\n",
        "from optuna.samplers import TPESampler\n",
        "import math\n",
        "\n",
        "torch.cuda.amp.autocast(enabled=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 15\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTSvAzOOlMl",
        "outputId": "7f7ca627-a3da-43f3-a9dc-a3178d422884"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f0abdd72830>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# tell pytorch to use cuda\n",
        "device = torch.device(\"cuda\")"
      ],
      "metadata": {
        "id": "Kp4e7RipOoTX"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Data preparation\n",
        "def create_input_text(df):\n",
        "  df.rename(columns={'input_text':'sentence'}, inplace=True)\n",
        "  df['input_text'] = df.apply(lambda row: row['prefix'] + \" : \" + row['sentence'], axis=1)\n",
        "  return df"
      ],
      "metadata": {
        "id": "POWPxTaIPUgx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Read data\n",
        "train_df = pd.read_csv(\"/content/drive/MyDrive/CS4NLP-HateXplain/data/t5_modeling/df_train.csv\").astype(str)[['prefix','input_text','target_text']]\n",
        "eval_df = pd.read_csv(\"/content/drive/MyDrive/CS4NLP-HateXplain/data/t5_modeling/df_val.csv\").astype(str)[['prefix','input_text','target_text']]\n",
        "test_df = pd.read_csv(\"/content/drive/MyDrive/CS4NLP-HateXplain/data/t5_modeling/df_test.csv\").astype(str)[['prefix','input_text','target_text']]\n",
        "\n",
        "\n",
        "## Output\n",
        "model_op_path = \"/content/drive/MyDrive/CS4NLP-HateXplain/data/t5_modeling/t5_ip_sent_v2/\"\n",
        "\n",
        "\n",
        "## Select data-points with prefix = 'label'\n",
        "train_df = train_df[train_df['prefix']=='label'].copy()\n",
        "eval_df = eval_df[eval_df['prefix']=='label'].copy()\n",
        "test_df = test_df[test_df['prefix']=='label'].copy()\n",
        "\n",
        "train_df.reset_index(drop=True, inplace=True)\n",
        "eval_df.reset_index(drop=True, inplace=True)\n",
        "test_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "## Create input_text with prefix\n",
        "create_input_text(train_df)\n",
        "create_input_text(eval_df)\n",
        "create_input_text(test_df)\n",
        "\n",
        "print(\"Train shape: \", train_df.shape)\n",
        "print(\"Val shape: \", eval_df.shape)\n",
        "print(\"Test shape: \", test_df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuvHeL6BOq3Y",
        "outputId": "4634951d-45f7-44ac-e983-124e461a1378"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train shape:  (14072, 4)\n",
            "Val shape:  (1787, 4)\n",
            "Test shape:  (1761, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiate Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained('t5-base')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208,
          "referenced_widgets": [
            "3faeb41982864a388903f016144bd411",
            "0001e9c191174cbf95b54e9c270d589c",
            "1e19468379f346b6aae2b3b858f865c3",
            "e19201fa135b4dc68bb28739b2e426ef",
            "98f6d3b49af0461496c0cc96c0041ddf",
            "4277637e1ec54cc2834a9d1f207038e0",
            "4c5c0bb9657a4b79b6c35f140ee9b178",
            "4ba67575acff465f8ecdde91db9b3176",
            "a1b365a5f39344638a904f570cd79945",
            "f239b8f041af482d9ea959e332023113",
            "5b0998509f0d4d16943b733d585306fa",
            "516a88f9de4848b5b1014ee1cfd9ed2b",
            "47abc5409d654a3eba35a6b14a1fcaf2",
            "0277b84b6b1f41bf90e02c26aaae32ff",
            "1061b007b7a94092af14d52ac14de938",
            "0903df6829d04d2b8e900d3ab6a2fcbb",
            "efe0671c5848433fb0bddc9d35fea4dc",
            "831a56ac485740d693453093319821b8",
            "8fc369fd0a104b9d9f7823e533ffadab",
            "27c1b42f280540a6adf055f5ae0f6176",
            "0495b72c3ccc44238e3d6c86999eb07a",
            "c5b847343fda40da8e2ed4150998318d"
          ]
        },
        "id": "9K-pwPJ6O7Mp",
        "outputId": "95206644-95d4-42ea-f986-976949156d6d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3faeb41982864a388903f016144bd411"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "516a88f9de4848b5b1014ee1cfd9ed2b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/t5/tokenization_t5.py:163: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenize the main text\n",
        "def tokenize_corpus(df, tokenizer, max_len=512):\n",
        "    # token ID storage\n",
        "    input_ids = []\n",
        "    # attension mask storage\n",
        "    attention_masks = []\n",
        "    # max len -- 512 is max\n",
        "    max_len = max_len\n",
        "    # for every document:\n",
        "    for doc in df:\n",
        "        # `encode_plus` will:\n",
        "        #   (1) Tokenize the sentence.\n",
        "        #   (2) Prepend the `[CLS]` token to the start.\n",
        "        #   (3) Append the `[SEP]` token to the end.\n",
        "        #   (4) Map tokens to their IDs.\n",
        "        #   (5) Pad or truncate the sentence to `max_length`\n",
        "        #   (6) Create attention masks for [PAD] tokens.\n",
        "        encoded_dict = tokenizer.encode_plus(\n",
        "                            doc,  # document to encode.\n",
        "                            add_special_tokens=True,  # add tokens relative to model\n",
        "                            max_length=max_len,  # set max length\n",
        "                            truncation=True,  # truncate longer messages\n",
        "                            pad_to_max_length=True,  # add padding\n",
        "                            return_attention_mask=True,  # create attn. masks\n",
        "                            return_tensors='pt'  # return pytorch tensors\n",
        "                       )\n",
        "\n",
        "        # add the tokenized sentence to the list\n",
        "        input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "        # and its attention mask (differentiates padding from non-padding)\n",
        "        attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n",
        "\n",
        "\n",
        "# create tokenized data - input_text\n",
        "train_body_input_ids, train_body_attention_masks = tokenize_corpus(train_df['input_text'].values, tokenizer)\n",
        "eval_body_input_ids, eval_body_attention_masks = tokenize_corpus(eval_df['input_text'].values, tokenizer)\n",
        "test_body_input_ids, test_body_attention_masks = tokenize_corpus(test_df['input_text'].values, tokenizer)\n",
        "\n",
        "# create tokenized data - target_text - max_len=2\n",
        "train_target_input_ids, train_target_attention_masks = tokenize_corpus(train_df['target_text'].values, tokenizer, max_len=2)\n",
        "eval_target_input_ids, eval_target_attention_masks = tokenize_corpus(eval_df['target_text'].values, tokenizer, max_len=2)\n",
        "test_target_input_ids, test_target_attention_masks = tokenize_corpus(test_df['target_text'].values, tokenizer, max_len=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OJ6Q5OQQYFm",
        "outputId": "67d99731-09df-4143-eaef-fc3a4b1a912e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:2377: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Function to prepare dataset\n",
        "def prepare_dataset(body_tokens, body_masks, target_token, target_masks):\n",
        "  tensor_df = TensorDataset(body_tokens, body_masks, target_token, target_masks)\n",
        "  return tensor_df\n",
        "\n",
        "# create tensor data sets\n",
        "train_dataset = prepare_dataset(train_body_input_ids, train_body_attention_masks, train_target_input_ids, train_target_attention_masks)\n",
        "eval_dataset = prepare_dataset(eval_body_input_ids, eval_body_attention_masks, eval_target_input_ids, eval_target_attention_masks)\n",
        "test_dataset = prepare_dataset(test_body_input_ids, test_body_attention_masks, test_target_input_ids, test_target_attention_masks)\n"
      ],
      "metadata": {
        "id": "OUjYIJVTRw0y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Instantiate training models\n",
        "## Training\n",
        "def train(model, dataloader, optimizer):\n",
        "\n",
        "    # capture time\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    # Perform one full pass over the training set.\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # reset total loss for epoch\n",
        "    train_total_loss = 0\n",
        "    total_train_f1 = 0\n",
        "\n",
        "    # put model into traning mode\n",
        "    model.train()\n",
        "\n",
        "    # for each batch of training data...\n",
        "    for step, batch in enumerate(dataloader):\n",
        "\n",
        "        # progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(dataloader)))\n",
        "\n",
        "        # Unpack this training batch from our dataloader:\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using\n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input tokens\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: target tokens\n",
        "        #   [3]: target attenion masks\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_input_mask = batch[1].cuda()\n",
        "        b_target_ids = batch[2].cuda()\n",
        "        b_target_mask = batch[3].cuda()\n",
        "\n",
        "        # clear previously calculated gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # runs the forward pass with autocasting.\n",
        "        with autocast():\n",
        "            # forward propagation (evaluate model on training batch)\n",
        "            outputs = model(input_ids=b_input_ids,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_target_ids,\n",
        "                            decoder_attention_mask=b_target_mask)\n",
        "\n",
        "            loss, prediction_scores = outputs[:2]\n",
        "\n",
        "            # sum the training loss over all batches for average loss at end\n",
        "            # loss is a tensor containing a single value\n",
        "            if math.isnan(loss.item())==False:\n",
        "              train_total_loss += loss.item()\n",
        "\n",
        "        # Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n",
        "        # Backward passes under autocast are not recommended.\n",
        "        # Backward ops run in the same dtype autocast chose for corresponding forward ops.\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        # Clip gradient\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
        "\n",
        "        # scaler.step() first unscales the gradients of the optimizer's assigned params.\n",
        "        # If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n",
        "        # otherwise, optimizer.step() is skipped.\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        # Updates the scale for next iteration.\n",
        "        scaler.update()\n",
        "\n",
        "        # update the learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "    # calculate the average loss over all of the batches\n",
        "    avg_train_loss = train_total_loss / len(dataloader)\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'Train Loss': avg_train_loss\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # training time end\n",
        "    training_time = format_time(time.time() - total_t0)\n",
        "\n",
        "    # print result summaries\n",
        "    print(\"\")\n",
        "    print(\"summary results\")\n",
        "    print(\"epoch | trn loss | trn time \")\n",
        "    print(f\"{epoch+1:5d} | {avg_train_loss:.5f} | {training_time:}\")\n",
        "\n",
        "    return training_stats\n"
      ],
      "metadata": {
        "id": "PqFmhNMvTmV5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Validation\n",
        "def validating(model, dataloader):\n",
        "\n",
        "    # capture validation time\n",
        "    total_t0 = time.time()\n",
        "\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # track variables\n",
        "    total_valid_loss = 0\n",
        "\n",
        "    # evaluate data for one epoch\n",
        "    for batch in dataloader:\n",
        "\n",
        "        # Unpack this training batch from our dataloader:\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input tokens\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: target tokens\n",
        "        #   [3]: target attenion masks\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_input_mask = batch[1].cuda()\n",
        "        b_target_ids = batch[2].cuda()\n",
        "        b_target_mask = batch[3].cuda()\n",
        "\n",
        "        # tell pytorch not to bother calculating gradients\n",
        "        # as its only necessary for training\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # forward propagation (evaluate model on training batch)\n",
        "            outputs = model(input_ids=b_input_ids,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_target_ids,\n",
        "                            decoder_attention_mask=b_target_mask)\n",
        "\n",
        "            loss, prediction_scores = outputs[:2]\n",
        "\n",
        "            # sum the training loss over all batches for average loss at end\n",
        "            # loss is a tensor containing a single value\n",
        "            total_valid_loss += loss.item()\n",
        "\n",
        "    # calculate the average loss over all of the batches.\n",
        "    global avg_val_loss\n",
        "    avg_val_loss = total_valid_loss / len(dataloader)\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    valid_stats.append(\n",
        "        {\n",
        "            'Val Loss': avg_val_loss,\n",
        "            'Val PPL.': np.exp(avg_val_loss)\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # capture end validation time\n",
        "    training_time = format_time(time.time() - total_t0)\n",
        "\n",
        "    # print result summaries\n",
        "    print(\"\")\n",
        "    print(\"summary results\")\n",
        "    print(\"epoch | val loss | val ppl | val time\")\n",
        "    print(f\"{epoch+1:5d} | {avg_val_loss:.5f} | {np.exp(avg_val_loss):.3f} | {training_time:}\")\n",
        "\n",
        "    return valid_stats\n"
      ],
      "metadata": {
        "id": "cfpVX97GXdcE"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Testing\n",
        "def testing(model, dataloader):\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Testing...\")\n",
        "\n",
        "    # measure training time\n",
        "    t0 = time.time()\n",
        "\n",
        "    # put the model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # track variables\n",
        "    total_test_loss = 0\n",
        "    total_test_acc = 0\n",
        "    total_test_f1 = 0\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    all_prediction_scores = []\n",
        "\n",
        "    # evaluate data for one epoch\n",
        "    for step, batch in enumerate(dataloader):\n",
        "        # progress update every 40 batches.\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader:\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input tokens\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: target tokens\n",
        "        #   [3]: target attenion masks\n",
        "        b_input_ids = batch[0].cuda()\n",
        "        b_input_mask = batch[1].cuda()\n",
        "        b_target_ids = batch[2].cuda()\n",
        "        b_target_mask = batch[3].cuda()\n",
        "\n",
        "        # tell pytorch not to bother calculating gradients\n",
        "        # as its only necessary for training\n",
        "        with torch.no_grad():\n",
        "\n",
        "            # forward propagation (evaluate model on training batch)\n",
        "            outputs = model(input_ids=b_input_ids,\n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_target_ids,\n",
        "                            decoder_attention_mask=b_target_mask)\n",
        "\n",
        "            loss, prediction_scores = outputs[:2]\n",
        "\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                    input_ids=b_input_ids,\n",
        "                    attention_mask=b_input_mask,\n",
        "                    max_length=3\n",
        "                    )\n",
        "\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True) for t in b_target_ids]\n",
        "\n",
        "            total_test_acc += accuracy_score(target, preds)\n",
        "            total_test_f1 += f1_score(preds, target,\n",
        "                                       average='weighted',\n",
        "                                       labels=np.unique(preds))\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "            all_prediction_scores.extend(prediction_scores)\n",
        "\n",
        "    # calculate the average loss over all of the batches.\n",
        "    avg_test_loss = total_test_loss / len(dataloader)\n",
        "\n",
        "    avg_test_acc = total_test_acc / len(test_dataloader)\n",
        "\n",
        "    avg_test_f1 = total_test_f1 / len(test_dataloader)\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    test_stats.append(\n",
        "        {\n",
        "            'Test Loss': avg_test_loss,\n",
        "            'Test PPL.': np.exp(avg_test_loss),\n",
        "            'Test Acc.': avg_test_acc,\n",
        "            'Test F1': avg_test_f1\n",
        "        }\n",
        "    )\n",
        "    global df2\n",
        "    temp_data = pd.DataFrame({'predicted': predictions, 'actual': actuals})\n",
        "    df2 = df2.append(temp_data)\n",
        "\n",
        "    return test_stats, all_prediction_scores\n"
      ],
      "metadata": {
        "id": "W4nNGXjuXl8C"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# time function\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "metadata": {
        "id": "aIotJ3JDYNxZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Prepare for training\n",
        "model = T5ForConditionalGeneration.from_pretrained('t5-base').cuda()  # to GPU\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=False)\n",
        "\n",
        "valid_dataloader = DataLoader(eval_dataset, batch_size=24, shuffle=False)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=24, shuffle=False)\n",
        "\n",
        "\n",
        "# Adam w/ Weight Decay Fix\n",
        "# set to optimizer_grouped_parameters or model.parameters()\n",
        "optimizer = AdamW(model.parameters(), lr = 1e-4)\n",
        "\n",
        "# epochs\n",
        "epochs = 5\n",
        "\n",
        "# lr scheduler\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
        "\n",
        "# create gradient scaler for mixed precision\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LymCYxfKYQLT",
        "outputId": "9ae78043-70d0-4497-91e9-aa187d88abdc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Empty cache\n",
        "\n",
        "with torch.no_grad():\n",
        "  torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "jazGlSvVyXrY"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create training result storage\n",
        "training_stats = []\n",
        "valid_stats = []\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "# for each epoch\n",
        "for epoch in range(epochs):\n",
        "    # train\n",
        "    train(model, train_dataloader, optimizer)\n",
        "    # validate\n",
        "    validating(model, valid_dataloader)\n",
        "    # check validation loss\n",
        "    if valid_stats[epoch]['Val Loss'] < best_valid_loss:\n",
        "        best_valid_loss = valid_stats[epoch]['Val Loss']\n",
        "        # save best model for use later\n",
        "        torch.save(model.state_dict(), model_op_path + 't5-classification.pt')  # torch save\n",
        "        model_to_save = model.module if hasattr(model, 'module') else model\n",
        "        model_to_save.save_pretrained(model_op_path + '/model_save/t5-classification/')  # transformers save\n",
        "        tokenizer.save_pretrained(model_op_path + '/model_save/t5-classification/')  # transformers save"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvWu8eRjZiYB",
        "outputId": "06a87e4d-eca1-4827-e844-fa33f41a7a4b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 5 ========\n",
            "Training...\n",
            "  Batch    40  of    880.\n",
            "  Batch    80  of    880.\n",
            "  Batch   120  of    880.\n",
            "  Batch   160  of    880.\n",
            "  Batch   200  of    880.\n",
            "  Batch   240  of    880.\n",
            "  Batch   280  of    880.\n",
            "  Batch   320  of    880.\n",
            "  Batch   360  of    880.\n",
            "  Batch   400  of    880.\n",
            "  Batch   440  of    880.\n",
            "  Batch   480  of    880.\n",
            "  Batch   520  of    880.\n",
            "  Batch   560  of    880.\n",
            "  Batch   600  of    880.\n",
            "  Batch   640  of    880.\n",
            "  Batch   680  of    880.\n",
            "  Batch   720  of    880.\n",
            "  Batch   760  of    880.\n",
            "  Batch   800  of    880.\n",
            "  Batch   840  of    880.\n",
            "\n",
            "summary results\n",
            "epoch | trn loss | trn time \n",
            "    1 | 1.33100 | 0:04:34\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "summary results\n",
            "epoch | val loss | val ppl | val time\n",
            "    1 | 0.40315 | 1.497 | 0:00:22\n",
            "\n",
            "======== Epoch 2 / 5 ========\n",
            "Training...\n",
            "  Batch    40  of    880.\n",
            "  Batch    80  of    880.\n",
            "  Batch   120  of    880.\n",
            "  Batch   160  of    880.\n",
            "  Batch   200  of    880.\n",
            "  Batch   240  of    880.\n",
            "  Batch   280  of    880.\n",
            "  Batch   320  of    880.\n",
            "  Batch   360  of    880.\n",
            "  Batch   400  of    880.\n",
            "  Batch   440  of    880.\n",
            "  Batch   480  of    880.\n",
            "  Batch   520  of    880.\n",
            "  Batch   560  of    880.\n",
            "  Batch   600  of    880.\n",
            "  Batch   640  of    880.\n",
            "  Batch   680  of    880.\n",
            "  Batch   720  of    880.\n",
            "  Batch   760  of    880.\n",
            "  Batch   800  of    880.\n",
            "  Batch   840  of    880.\n",
            "\n",
            "summary results\n",
            "epoch | trn loss | trn time \n",
            "    2 | 0.48183 | 0:04:31\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "summary results\n",
            "epoch | val loss | val ppl | val time\n",
            "    2 | 0.43775 | 1.549 | 0:00:22\n",
            "\n",
            "======== Epoch 3 / 5 ========\n",
            "Training...\n",
            "  Batch    40  of    880.\n",
            "  Batch    80  of    880.\n",
            "  Batch   120  of    880.\n",
            "  Batch   160  of    880.\n",
            "  Batch   200  of    880.\n",
            "  Batch   240  of    880.\n",
            "  Batch   280  of    880.\n",
            "  Batch   320  of    880.\n",
            "  Batch   360  of    880.\n",
            "  Batch   400  of    880.\n",
            "  Batch   440  of    880.\n",
            "  Batch   480  of    880.\n",
            "  Batch   520  of    880.\n",
            "  Batch   560  of    880.\n",
            "  Batch   600  of    880.\n",
            "  Batch   640  of    880.\n",
            "  Batch   680  of    880.\n",
            "  Batch   720  of    880.\n",
            "  Batch   760  of    880.\n",
            "  Batch   800  of    880.\n",
            "  Batch   840  of    880.\n",
            "\n",
            "summary results\n",
            "epoch | trn loss | trn time \n",
            "    3 | 0.50008 | 0:04:30\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "summary results\n",
            "epoch | val loss | val ppl | val time\n",
            "    3 | 0.43185 | 1.540 | 0:00:22\n",
            "\n",
            "======== Epoch 4 / 5 ========\n",
            "Training...\n",
            "  Batch    40  of    880.\n",
            "  Batch    80  of    880.\n",
            "  Batch   120  of    880.\n",
            "  Batch   160  of    880.\n",
            "  Batch   200  of    880.\n",
            "  Batch   240  of    880.\n",
            "  Batch   280  of    880.\n",
            "  Batch   320  of    880.\n",
            "  Batch   360  of    880.\n",
            "  Batch   400  of    880.\n",
            "  Batch   440  of    880.\n",
            "  Batch   480  of    880.\n",
            "  Batch   520  of    880.\n",
            "  Batch   560  of    880.\n",
            "  Batch   600  of    880.\n",
            "  Batch   640  of    880.\n",
            "  Batch   680  of    880.\n",
            "  Batch   720  of    880.\n",
            "  Batch   760  of    880.\n",
            "  Batch   800  of    880.\n",
            "  Batch   840  of    880.\n",
            "\n",
            "summary results\n",
            "epoch | trn loss | trn time \n",
            "    4 | 0.49654 | 0:04:31\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "summary results\n",
            "epoch | val loss | val ppl | val time\n",
            "    4 | 0.43246 | 1.541 | 0:00:22\n",
            "\n",
            "======== Epoch 5 / 5 ========\n",
            "Training...\n",
            "  Batch    40  of    880.\n",
            "  Batch    80  of    880.\n",
            "  Batch   120  of    880.\n",
            "  Batch   160  of    880.\n",
            "  Batch   200  of    880.\n",
            "  Batch   240  of    880.\n",
            "  Batch   280  of    880.\n",
            "  Batch   320  of    880.\n",
            "  Batch   360  of    880.\n",
            "  Batch   400  of    880.\n",
            "  Batch   440  of    880.\n",
            "  Batch   480  of    880.\n",
            "  Batch   520  of    880.\n",
            "  Batch   560  of    880.\n",
            "  Batch   600  of    880.\n",
            "  Batch   640  of    880.\n",
            "  Batch   680  of    880.\n",
            "  Batch   720  of    880.\n",
            "  Batch   760  of    880.\n",
            "  Batch   800  of    880.\n",
            "  Batch   840  of    880.\n",
            "\n",
            "summary results\n",
            "epoch | trn loss | trn time \n",
            "    5 | 0.49918 | 0:04:31\n",
            "\n",
            "Running Validation...\n",
            "\n",
            "summary results\n",
            "epoch | val loss | val ppl | val time\n",
            "    5 | 0.43246 | 1.541 | 0:00:22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test the model\n",
        "df2 = pd.DataFrame({'predicted': [], 'actual': []})\n",
        "test_stats = []\n",
        "model.load_state_dict(torch.load(model_op_path + 't5-classification.pt'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOqhsCilh7Tm",
        "outputId": "bc1776d8-af1f-4111-b117-de534f60fa02"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_stats, all_prediction_scores = testing(model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjxRXPtkjU7M",
        "outputId": "0ed5ba37-b5f6-44c7-8bef-3b792557d923"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running Testing...\n",
            "  Batch    40  of     74.    Elapsed: 0:00:28.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-18-1ad616789eb2>:90: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df2 = df2.append(temp_data)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_stats, df2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KntbkKCpjdjQ",
        "outputId": "d633c644-8a61-4764-9fbf-1d136fa82d80"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([{'Test Loss': 0.3919868980710571,\n",
              "   'Test PPL.': 1.4799183214905764,\n",
              "   'Test Acc.': 0.6672297297297295,\n",
              "   'Test F1': 0.681737862677551}],\n",
              "       predicted     actual\n",
              " 0        normal     normal\n",
              " 1          hate       hate\n",
              " 2        normal     normal\n",
              " 3     offensive  offensive\n",
              " 4          hate  offensive\n",
              " ...         ...        ...\n",
              " 1756     normal     normal\n",
              " 1757     normal  offensive\n",
              " 1758     normal     normal\n",
              " 1759     normal     normal\n",
              " 1760  offensive  offensive\n",
              " \n",
              " [1761 rows x 2 columns])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Softmax function\n",
        "def softmax(x):\n",
        "    max = np.max(x,axis=1,keepdims=True) #returns max of each row and keeps same dims\n",
        "    e_x = np.exp(x - max) #subtracts each row with its max value\n",
        "    sum = np.sum(e_x,axis=1,keepdims=True) #returns sum of each row and keeps same dims\n",
        "    f_x = e_x / sum\n",
        "    return f_x\n",
        "\n",
        "\n",
        "## Offensive token = 12130\n",
        "## Normal token = 1389\n",
        "## Hate token = 5591\n",
        "\n",
        "# Function to get probabilities\n",
        "# all prediction scores = list of logits of len(test_df)\n",
        "# ith item in all_prediction_scores contain 2 tensors - logits for first word, and logits for second word over 32K tokens.\n",
        "# This function extracts the logits for normal, offensive and hate, and converts them to probabilities by using a softmax\n",
        "def convert_to_prob(all_prediction_scores):\n",
        "  probs = np.zeros((len(all_prediction_scores), 3))\n",
        "  for i in range(len(all_prediction_scores)):\n",
        "    ## extract logits for normal, hate and offensive\n",
        "    offensive_logit = all_prediction_scores[i][0][12130]\n",
        "    normal_logit = all_prediction_scores[i][0][1389]\n",
        "    hate_logit = all_prediction_scores[i][0][5591]\n",
        "    probs[i][0] = normal_logit\n",
        "    probs[i][1] = offensive_logit\n",
        "    probs[i][2] = hate_logit\n",
        "  probs_softmax = softmax(probs)\n",
        "  df_probs = pd.DataFrame(probs_softmax, columns=['normal','offensive','hate'])\n",
        "  return df_probs\n"
      ],
      "metadata": {
        "id": "nS0EaOG0jzPx"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert logits to probability\n",
        "df_probs = convert_to_prob(all_prediction_scores)\n",
        "# Save probabilities\n",
        "df_probs.to_csv(model_op_path + 'test_probabilities.csv', index=False)\n",
        "df_probs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ERcRQ5p1uk0g",
        "outputId": "d2ac3a15-a01e-49dd-81f3-713cd8c6284a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     normal  offensive      hate\n",
              "0  0.807851   0.119638  0.072511\n",
              "1  0.031329   0.012176  0.956495\n",
              "2  0.372776   0.293188  0.334036\n",
              "3  0.135892   0.757348  0.106761\n",
              "4  0.187999   0.311373  0.500627"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-31590832-e01b-4c37-92bc-94b5075fe185\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>normal</th>\n",
              "      <th>offensive</th>\n",
              "      <th>hate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.807851</td>\n",
              "      <td>0.119638</td>\n",
              "      <td>0.072511</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.031329</td>\n",
              "      <td>0.012176</td>\n",
              "      <td>0.956495</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.372776</td>\n",
              "      <td>0.293188</td>\n",
              "      <td>0.334036</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.135892</td>\n",
              "      <td>0.757348</td>\n",
              "      <td>0.106761</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.187999</td>\n",
              "      <td>0.311373</td>\n",
              "      <td>0.500627</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-31590832-e01b-4c37-92bc-94b5075fe185')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-31590832-e01b-4c37-92bc-94b5075fe185 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-31590832-e01b-4c37-92bc-94b5075fe185');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = (df2['predicted'].apply(lambda x: x.strip()) == df2['actual'].apply(lambda x: x.strip())).sum()\n",
        "acc = correct/df2.shape[0]\n",
        "acc"
      ],
      "metadata": {
        "id": "VMU1VEd_uwAS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "495aa48c-5380-449c-a4eb-8fc0ceaaba24"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6672345258375922"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h8ErEBs55JJS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}