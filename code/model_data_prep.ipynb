{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb23261",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Code to prep data for t5 modeling. \n",
    "I/p to t5: <prefix>:<sentence>\n",
    "I/p to bert: <sentence> [SEP] <hugchat explanation> [SEP] <keywords> (not sure about keywords) -> Predict hate_speech\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7c808b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f4dbbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Input \n",
    "train_processed_path = '../data/rationale_extraction/df_train_rationale_post_processed.csv'\n",
    "val_processed_path = '../data/rationale_extraction/df_val_rationale_post_processed.csv'\n",
    "test_processed_path = '../data/rationale_extraction/df_test_rationale_post_processed.csv'\n",
    "\n",
    "### Output \n",
    "train_t5_path = '../data/t5_modeling/df_train.csv'\n",
    "val_t5_path = '../data/t5_modeling/df_val.csv'\n",
    "test_t5_path = '../data/t5_modeling/df_test.csv'\n",
    "\n",
    "train_bert_path = '../data/bert_modeling/df_train.csv'\n",
    "val_bert_path = '../data/bert_modeling/df_val.csv'\n",
    "test_bert_path = '../data/bert_modeling/df_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "faf58dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    df = pd.read_csv(path)\n",
    "    label_mapper = {'hate_speech':'hate', 'normal_speech':'normal', 'offensive_speech':'offensive'}\n",
    "    df['hugchat_label_processed'] = df['hugchat_label_processed'].map(label_mapper)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb0c70b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_helper(annotator_response):\n",
    "    labels = ast.literal_eval(annotator_response)['label']\n",
    "    count_hate = labels.count(0)\n",
    "    count_offensive = labels.count(2)\n",
    "    count_normal = labels.count(1)\n",
    "    if count_hate==count_offensive==count_normal:\n",
    "        return \"discard\"\n",
    "    elif count_hate > max(count_offensive, count_normal):\n",
    "        return \"hate\"\n",
    "    elif count_offensive > max(count_hate, count_normal):\n",
    "        return \"offensive\"\n",
    "    elif count_normal > max(count_hate, count_offensive):\n",
    "        return \"normal\"\n",
    "    else:\n",
    "        return \"discard\"\n",
    "\n",
    "# 0 = Hate, 1 = normal, 2 = offensive \n",
    "def get_label(df):\n",
    "    df['gt_label'] = df['annotators'].apply(lambda x: get_label_helper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62777a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comm_target_helper(annotator_response):\n",
    "    comm_target = ast.literal_eval(annotator_response)['target']\n",
    "    comm_target_all = set()\n",
    "    for tgt in comm_target:\n",
    "        for comm in tgt: \n",
    "            if comm not in comm_target_all:\n",
    "                comm_target_all.add(comm)\n",
    "    return list(comm_target_all)\n",
    "    \n",
    "# 0 = Hate, 1 = normal, 2 = offensive \n",
    "def get_comm_target(df):\n",
    "    df['gt_comm_target'] = df['annotators'].apply(lambda x: get_comm_target_helper(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bec90c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rationales_helper(rationales, tokens):\n",
    "    rationales = ast.literal_eval(rationales)\n",
    "    if len(rationales)==0:\n",
    "        return []\n",
    "    ratnl_sum = np.array(rationales).sum(axis=0).tolist()\n",
    "    ind = [i for i in range(len(ratnl_sum)) if ratnl_sum[i]>0]\n",
    "    tokens = ast.literal_eval(tokens)\n",
    "    rational_tokens = [tokens[i] for i in ind]\n",
    "    return rational_tokens\n",
    "    \n",
    "def get_rationales(df):\n",
    "    df['gt_keywords'] = df[['rationales','post_tokens']].apply(lambda row: get_rationales_helper(row['rationales'], row['post_tokens']), axis=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76db6384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create column with mixed community target \n",
    "# if there are no keywords in gt_rationales, we pick keywords from hugchat_keywords_processed\n",
    "def create_mix_keywords(df):\n",
    "    df['mix_keywords'] = df['gt_keywords'].copy()\n",
    "    no_kw_idx = df['mix_keywords'].apply(lambda x: len(x)==0)\n",
    "    df['mix_keywords'][no_kw_idx] = df['hugchat_keywords_processed'][no_kw_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56c58f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T5 data should consist of <prefix> <sentence>\n",
    "# <label> <sentence> - classification task to predict hate speech label\n",
    "# <explanation> <sentence> - generate free-text explanation \n",
    "# <keywords> <sentence> - generate keywords \n",
    "# <comm target> <sentence> - comm. target  \n",
    "def create_t5_data(df):\n",
    "    # hate speech classification \n",
    "    df_classification = df[['id','unmasked_sentence','gt_label']].copy()\n",
    "    df_classification['prefix'] = 'label'\n",
    "    df_classification = df_classification[['id','prefix','unmasked_sentence','gt_label']].copy()\n",
    "    df_classification.rename(columns = {'unmasked_sentence':'input_text', 'gt_label':'target_text'}, inplace=True)\n",
    "    print(\"Classification shape: \", df_classification.shape)\n",
    "    \n",
    "    # generate explanation \n",
    "    df_explanation = df[['id','unmasked_sentence','hugchat_explanation']].copy()\n",
    "    df_explanation['prefix'] = 'explanation'\n",
    "    drop_row_idx = df['hugchat_explanation'][df['hugchat_explanation'].apply(str).apply(lambda x: len(x)<=10)].index\n",
    "    df_explanation.drop(index=drop_row_idx, inplace=True)\n",
    "    df_explanation.reset_index(drop=True, inplace=True)\n",
    "    df_explanation = df_explanation[['id','prefix','unmasked_sentence','hugchat_explanation']].copy()\n",
    "    df_explanation.rename(columns = {'unmasked_sentence':'input_text', 'hugchat_explanation':'target_text'}, inplace=True)\n",
    "    print(\"Explanation shape: \", df_explanation.shape)\n",
    "    \n",
    "    # get communities targeted\n",
    "    df_comm_target = df[['id','unmasked_sentence','gt_comm_target']].copy()\n",
    "    df_comm_target['prefix'] = 'comm_target'\n",
    "    df_comm_target['gt_comm_target'] = df_comm_target['gt_comm_target'].apply(lambda x: ' '.join(k for k in x))\n",
    "    df_comm_target = df_comm_target[['id','prefix','unmasked_sentence','gt_comm_target']].copy()\n",
    "    df_comm_target.rename(columns = {'unmasked_sentence':'input_text', 'gt_comm_target':'target_text'}, inplace=True)\n",
    "    print(\"Comm Target shape: \", df_comm_target.shape)\n",
    "    \n",
    "    # get keywords \n",
    "    df_kw = df[['id','unmasked_sentence', 'mix_keywords']].copy()\n",
    "    df_kw['prefix'] = 'keywords'\n",
    "    df_kw['mix_keywords'] = df_kw['mix_keywords'].apply(str).apply(lambda x: ' '.join(k for k in ast.literal_eval(x)))\n",
    "    df_kw = df_kw[['id','prefix','unmasked_sentence','mix_keywords']].copy()\n",
    "    df_kw.rename(columns = {'unmasked_sentence':'input_text', 'mix_keywords':'target_text'}, inplace=True)\n",
    "    print(\"Keywords shape: \", df_comm_target.shape)\n",
    "    \n",
    "    df_t5 = pd.concat([df_classification, df_explanation, df_comm_target, df_kw], ignore_index=True)\n",
    "    print(\"Final shape: \", df_t5.shape)\n",
    "    df_t5 = df_t5.sample(frac=1, random_state=42)\n",
    "    df_t5.reset_index(drop=True, inplace=True)\n",
    "    print(\"\\n\")\n",
    "    return df_t5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59681f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "## create bert data \n",
    "## objective: predict hate speech and communities targeted from sentence, hugchat rationale and keywords \n",
    "def create_bert_data(df):\n",
    "    df_bert = df[['id','unmasked_sentence','hugchat_keywords_processed','hugchat_explanation','mix_keywords','gt_comm_target','gt_label']].copy()\n",
    "    drop_row_idx = df['hugchat_explanation'][df['hugchat_explanation'].apply(str).apply(lambda x: len(x)<=10)].index\n",
    "    df_bert.drop(index=drop_row_idx, inplace=True)\n",
    "    df_bert.reset_index(drop=True, inplace=True)\n",
    "    print(\"Bert data shape: \", df_bert.shape)\n",
    "    return df_bert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f34c4492",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(path):\n",
    "    df = load_data(path)\n",
    "    get_label(df)\n",
    "    get_comm_target(df)\n",
    "    get_rationales(df)\n",
    "    create_mix_keywords(df)\n",
    "    df_t5 = create_t5_data(df)\n",
    "    df_bert = create_bert_data(df)\n",
    "    return df_t5, df_bert  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9833ff3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification shape:  (14072, 4)\n",
      "Explanation shape:  (14057, 4)\n",
      "Comm Target shape:  (14072, 4)\n",
      "Keywords shape:  (14072, 4)\n",
      "Final shape:  (56273, 4)\n",
      "\n",
      "\n",
      "Bert data shape:  (14057, 7)\n",
      "Classification shape:  (1787, 4)\n",
      "Explanation shape:  (1786, 4)\n",
      "Comm Target shape:  (1787, 4)\n",
      "Keywords shape:  (1787, 4)\n",
      "Final shape:  (7147, 4)\n",
      "\n",
      "\n",
      "Bert data shape:  (1786, 7)\n",
      "Classification shape:  (1761, 4)\n",
      "Explanation shape:  (1759, 4)\n",
      "Comm Target shape:  (1761, 4)\n",
      "Keywords shape:  (1761, 4)\n",
      "Final shape:  (7042, 4)\n",
      "\n",
      "\n",
      "Bert data shape:  (1759, 7)\n"
     ]
    }
   ],
   "source": [
    "df_t5_train, df_bert_train = run(train_processed_path)\n",
    "df_t5_val, df_bert_val = run(val_processed_path)\n",
    "df_t5_test, df_bert_test = run(test_processed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c5a17a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save the outputs \n",
    "df_t5_train.to_csv(train_t5_path, index=False)\n",
    "df_t5_val.to_csv(val_t5_path, index=False)\n",
    "df_t5_test.to_csv(test_t5_path, index=False)\n",
    "\n",
    "df_bert_train.to_csv(train_bert_path, index=False)\n",
    "df_bert_val.to_csv(val_bert_path, index=False)\n",
    "df_bert_test.to_csv(test_bert_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba6e584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
